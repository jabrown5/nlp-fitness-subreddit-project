{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "going-brave",
   "metadata": {},
   "source": [
    "# Data Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "import datetime as dt\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-zambia",
   "metadata": {},
   "source": [
    "### Gather data using the requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wt_df = pd.DataFrame()\n",
    "wt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating weight training dataframe\n",
    "wt_df = pd.DataFrame()\n",
    "\n",
    "def getPushshiftData(before, subreddit):\n",
    "    params = {\n",
    "    'subreddit' : subreddit,\n",
    "    'size' : 3,\n",
    "    'before' : before\n",
    "    }\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "    \n",
    "#     url = 'https://api.pushshift.io/reddit/search/submission?&size=1000&after='+str(after)+'&subreddit='+str(sub)\n",
    "#     r = requests.get(url)\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "#     data = res.json()\n",
    "    posts = data['data']\n",
    "\n",
    "#     data = json.loads(res.text)\n",
    "#     return data['data']\n",
    "#     print(posts)\n",
    "#     df = pd.DataFrame(posts)\n",
    "#     wt_df = pd.concat([wt_df, df])\n",
    "    return data['data']\n",
    "\n",
    "#list of post ID's\n",
    "post_ids = []\n",
    "#Subreddit to query\n",
    "subreddit='WeightTraining'\n",
    "# Unix timestamp of date to crawl from.\n",
    "# 2018/04/01\n",
    "\n",
    "# https://www.epochconverter.com/\n",
    "before = '1615729460'    \n",
    "# after = \"1522618956\"\n",
    "\n",
    "data = getPushshiftData(before, subreddit)\n",
    "\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'before' date up until todays date\n",
    "while len(data) < 15:\n",
    "    for submission in data:\n",
    "        post_ids.append(submission)\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    data = getPushshiftData(subreddit=subreddit, before=data[-1]['created_utc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-clone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-return",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(after, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission?&size=1000&after='+str(after)+'&subreddit='+str(sub)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "#list of post ID's\n",
    "post_ids = []\n",
    "#Subreddit to query\n",
    "sub='btc'\n",
    "# Unix timestamp of date to crawl from.\n",
    "# 2018/04/01\n",
    "after = \"1522618956\"\n",
    "\n",
    "data = getPushshiftData(after, sub)\n",
    "\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until todays date\n",
    "while len(data) <201:\n",
    "    for submission in data:\n",
    "        post_ids.append(submission[\"id\"])\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    data = getPushshiftData(sub=sub, after=data[-1]['created_utc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating weight training dataframe\n",
    "\n",
    "def getPushshiftData(before, subreddit):\n",
    "    params = {\n",
    "    'subreddit' : subreddit,\n",
    "    'size' : 2,\n",
    "    'before' : before\n",
    "    }\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "    #print(f'status code: {res.status_code}')\n",
    "    data = res.json()\n",
    "    \n",
    "    posts = data['data']\n",
    "    #print(posts[-1]['created_utc'])\n",
    "    df = pd.DataFrame(posts)\n",
    "#     wt_df = wt_df\n",
    "#     wt_df = pd.concat([wt_df, df])\n",
    "    print(len(df.index))\n",
    "    return len(df.index)\n",
    "\n",
    "subreddit='WeightTraining'\n",
    "before = '1615729460'    \n",
    "\n",
    "\n",
    "data = getPushshiftData(before, subreddit)\n",
    "\n",
    "\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until todays date\n",
    "while len(df) < 300:\n",
    "    df = pd.concat([df, df])\n",
    "    for submission in data:\n",
    "        post_ids.append(submission[\"id\"])\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    data = getPushshiftData(subreddit=subreddit, before=df[-1:]['created_utc'])\n",
    "\n",
    "# #list of post ID's\n",
    "# # post_list = []\n",
    "# #Subreddit to query\n",
    "# subreddit='WeightTraining'\n",
    "# # Unix timestamp of date to crawl from.\n",
    "# # 2018/04/01\n",
    "# before = '1615729460'    \n",
    "# # after = \"1522618956\"\n",
    "\n",
    "\n",
    "#list of post ID's\n",
    "#post_ids = []\n",
    "#Subreddit to query\n",
    "# sub='btc'\n",
    "# Unix timestamp of date to crawl from.\n",
    "# 2018/04/01\n",
    "# after = \"1522618956\"\n",
    "\n",
    "# data = getPushshiftData(after, sub)\n",
    "\n",
    "# # Will run until all posts have been gathered \n",
    "# # from the 'after' date up until todays date\n",
    "# while len(data) < 14:\n",
    "#     for submission in data:\n",
    "#         post_ids.append(submission[\"id\"])\n",
    "#     # Calls getPushshiftData() with the created date of the last submission\n",
    "#     data = getPushshiftData(subreddit=subreddit, before=data[-1]['created_utc'])\n",
    "    \n",
    "\n",
    "# # https://www.epochconverter.com/\n",
    "# before = '1615729460'    \n",
    "# # after = \"1522618956\"\n",
    "\n",
    "# posts = getPushshiftData(before, subreddit)\n",
    "\n",
    "# # Will run until all posts have been gathered \n",
    "# # from the 'before' date up until todays date\n",
    "# while len(posts) < 15:\n",
    "#     for submission in posts:\n",
    "#         post_list.append(submission)\n",
    "#     # Calls getPushshiftData() with the created date of the last submission\n",
    "#     posts = getPushshiftData(subreddit=subreddit, before=data[-1]['created_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "def get_posts(subreddit, last):\n",
    "\n",
    "\t\"\"\"Very slightly modified from a script created by Alex P.\n",
    "\tFound at https://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n",
    "\tCrawl a page of results from a given subreddit.\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param last: The last downloaded page.\n",
    "    :return: A page or results.\n",
    "    \"\"\"\n",
    "\n",
    "\tparams = {\n",
    "\t'subreddit' : subreddit,\n",
    "\t'size' : 2,\n",
    "\t'sort' : 'desc',\n",
    "\t'sort_type' : 'created_utc'\n",
    "\t}\n",
    "\n",
    "\tif last != None:\n",
    "\t\tif len(last) > 0:\n",
    "\t\t\tparams['before'] = last[-1]['created_utc']\n",
    "\t\telse:\n",
    "\t\t\treturn []\n",
    "\n",
    "\tresults = requests.get(url, params)\n",
    "\n",
    "\treturn results.json()['data']\n",
    "\n",
    "\n",
    "def get_num_total_posts(subreddit, num_total_posts):\n",
    "\t\"\"\"\n",
    "  \tAlso found from the same link as 'get_posts'\n",
    "  \tVariable names are changed from original for my usage.\n",
    "  \tCrawl submissions from a subreddit.\n",
    "  \t:param subreddit: The subreddit to crawl.\n",
    "  \t:param num_total_posts: The maximum number of submissions to download.\n",
    "  \t:return: A list of submissions.\n",
    "  \t\"\"\"\n",
    "\n",
    "\tsubmissions = []\n",
    "\tlast = None\n",
    "\n",
    "\twhile last != [] and len(submissions) < num_total_posts:\n",
    "\t\tlast = get_posts(subreddit, last)\n",
    "\t\tsubmissions += last\n",
    "# \t\ttime.sleep(3)\n",
    "\treturn submissions[:num_total_posts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "continuous-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "def scraper(subreddit, last_batch):\n",
    "    # setting up params to indicate what subreddit\n",
    "    # size (max is 100)\n",
    "    # excluding videos\n",
    "    params = {\n",
    "    'subreddit' : subreddit,\n",
    "    'size' : 25,\n",
    "    'is_video': False,\n",
    "    'author': '!AutoModerator',\n",
    "    'user_removed': True,\n",
    "    'mod_removed': True\n",
    "#         ,\n",
    "#     'sort' : 'desc',\n",
    "#     'sort_type' : 'created_utc'\n",
    "    }\n",
    "    # if the last_batch of scraped submissions is NOT blank:\n",
    "    if last_batch != None:\n",
    "        # Find the last row of the most recent scrape to pull the \n",
    "        # UTC time in order to know where to initiate the next\n",
    "        # scrape\n",
    "        if len(last_batch) > 0:\n",
    "            # creating the param 'before' = to be included in request.get call below\n",
    "            params['before'] = last_batch[-1]['created_utc']\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # Make the request with url and params that have been updated for each scrape    \n",
    "    res = requests.get(url, params)\n",
    "    # return json data\n",
    "    return res.json()['data']\n",
    "\n",
    "\n",
    "def compiler(subreddit, posts_to_scrape):\n",
    "\n",
    "    # creating a list to store retrieved posts\n",
    "    post_list = []\n",
    "    \n",
    "    # setting up a list to hold the scraped posts from the \n",
    "    # most recent post scrape interation\n",
    "    last_batch = None\n",
    "\n",
    "    # settin up while look that will run as long as last_batch is NOT blank\n",
    "    # AND while the length of the post_list is still less than the number\n",
    "    # of desired posts to scrape\n",
    "    while last_batch != [] and len(post_list) < posts_to_scrape:\n",
    "        # calling the scraper() function and saving it as last_batch\n",
    "        last_batch = scraper(subreddit, last_batch)\n",
    "        # adding the most recent batch of scraped submissions to the\n",
    "        # larger post_list list\n",
    "        post_list += last_batch\n",
    "#         time.sleep(3)\n",
    "        # setting up .sleep() to randomly pick a wait time so\n",
    "        # scrape seems more 'natural'\n",
    "        time.sleep(random.randint(3,7))\n",
    "        \n",
    "    return post_list[:posts_to_scrape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "broken-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = compiler('bodyweightfitness', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "seasonal-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = compiler('weightlifting', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "silent-dominican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(bw)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "rocky-render",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 74)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = pd.DataFrame(wl)\n",
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "explicit-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 76)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = pd.concat([test, test2], axis=0, sort=False)\n",
    "test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(retrieved_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "active-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_scrapes(df1, df2):\n",
    "    \n",
    "    df1 = pd.DataFrame(df1)\n",
    "    df2 = pd.DataFrame(df2)\n",
    "    df = pd.concat([df1, df2], axis=0, sort=False)\n",
    "\n",
    "    # dropping duplicates\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "\n",
    "#     # dropping postes that have been removed via a\n",
    "#     # moderator, user deleted, or by reddit\n",
    "#     df = df[df['removed_by_category'] != 'moderator' ]\n",
    "#     df = df[df['removed_by_category'] != 'deleted' ]\n",
    "#     df = df[df['removed_by_category'] != 'reddit' ]\n",
    "\n",
    "#     df2 = df2[df2['removed_by_category'] != 'moderator' ]\n",
    "#     df2 = df2[df2['removed_by_category'] != 'deleted' ]\n",
    "#     df2 = df2[df2['removed_by_category'] != 'reddit' ]\n",
    "    \n",
    "#     # keeping only posts with text in selftext\n",
    "#     df1 = df1.loc[df1['selftext'].str.len() > 0]\n",
    "    df = df.loc[df['selftext'].str.len() > 0]\n",
    "\n",
    "#     # merging the dataframes into 1\n",
    "#     df = pd.concat([df1, df2], axis=0, sort=False)\n",
    "\n",
    "    export_df = df.to_csv('../data/compiled_final3.csv', index = False) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "lucky-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = compile_scrapes(test, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "handled-burst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 76)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "growing-denial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>awarders</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>full_link</th>\n",
       "      <th>gildings</th>\n",
       "      <th>id</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media_only</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>pwls</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>url_overridden_by_dest</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Competitive-Book1027</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_9rurgo1o</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1615744149</td>\n",
       "      <td>self.bodyweightfitness</td>\n",
       "      <td>https://www.reddit.com/r/bodyweightfitness/com...</td>\n",
       "      <td>{}</td>\n",
       "      <td>m4zziq</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>/r/bodyweightfitness/comments/m4zziq/wanting_t...</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>moderator</td>\n",
       "      <td>1615744159</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "      <td>t5_2tf0a</td>\n",
       "      <td>2026909</td>\n",
       "      <td>public</td>\n",
       "      <td>self</td>\n",
       "      <td>Wanting to get started!</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/bodyweightfitness/com...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>isikio</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_896srifv</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1615743524</td>\n",
       "      <td>self.bodyweightfitness</td>\n",
       "      <td>https://www.reddit.com/r/bodyweightfitness/com...</td>\n",
       "      <td>{}</td>\n",
       "      <td>m4zrg5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>/r/bodyweightfitness/comments/m4zrg5/from_tuck...</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>moderator</td>\n",
       "      <td>1615743534</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "      <td>t5_2tf0a</td>\n",
       "      <td>2026902</td>\n",
       "      <td>public</td>\n",
       "      <td>self</td>\n",
       "      <td>From tuck to straddle planche in 6 months real...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/bodyweightfitness/com...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments                author  \\\n",
       "0            []                False  Competitive-Book1027   \n",
       "1            []                False                isikio   \n",
       "\n",
       "  author_flair_css_class author_flair_richtext author_flair_text  \\\n",
       "0                   None                    []              None   \n",
       "1                   None                    []              None   \n",
       "\n",
       "  author_flair_type author_fullname author_patreon_flair author_premium  \\\n",
       "0              text     t2_9rurgo1o                False          False   \n",
       "1              text     t2_896srifv                False          False   \n",
       "\n",
       "  awarders  can_mod_post  contest_mode  created_utc                  domain  \\\n",
       "0       []         False         False   1615744149  self.bodyweightfitness   \n",
       "1       []         False         False   1615743524  self.bodyweightfitness   \n",
       "\n",
       "                                           full_link gildings      id  \\\n",
       "0  https://www.reddit.com/r/bodyweightfitness/com...       {}  m4zziq   \n",
       "1  https://www.reddit.com/r/bodyweightfitness/com...       {}  m4zrg5   \n",
       "\n",
       "   is_crosspostable  is_meta  is_original_content  is_reddit_media_domain  \\\n",
       "0             False    False                False                   False   \n",
       "1             False    False                False                   False   \n",
       "\n",
       "   is_robot_indexable  is_self  is_video link_flair_background_color  \\\n",
       "0               False     True     False                               \n",
       "1               False     True     False                               \n",
       "\n",
       "  link_flair_richtext link_flair_text_color link_flair_type  locked  \\\n",
       "0                  []                  dark            text   False   \n",
       "1                  []                  dark            text   False   \n",
       "\n",
       "   media_only  no_follow  num_comments  num_crossposts  over_18  \\\n",
       "0       False       True             0               0    False   \n",
       "1       False       True             0               0    False   \n",
       "\n",
       "  parent_whitelist_status                                          permalink  \\\n",
       "0                 all_ads  /r/bodyweightfitness/comments/m4zziq/wanting_t...   \n",
       "1                 all_ads  /r/bodyweightfitness/comments/m4zrg5/from_tuck...   \n",
       "\n",
       "   pinned  pwls removed_by_category  retrieved_on  score   selftext  \\\n",
       "0   False     6           moderator    1615744159      1  [removed]   \n",
       "1   False     6           moderator    1615743534      1  [removed]   \n",
       "\n",
       "   send_replies  spoiler  stickied          subreddit subreddit_id  \\\n",
       "0          True    False     False  bodyweightfitness     t5_2tf0a   \n",
       "1          True    False     False  bodyweightfitness     t5_2tf0a   \n",
       "\n",
       "   subreddit_subscribers subreddit_type thumbnail  \\\n",
       "0                2026909         public      self   \n",
       "1                2026902         public      self   \n",
       "\n",
       "                                               title  total_awards_received  \\\n",
       "0                            Wanting to get started!                      0   \n",
       "1  From tuck to straddle planche in 6 months real...                      0   \n",
       "\n",
       "  treatment_tags  upvote_ratio  \\\n",
       "0             []           1.0   \n",
       "1             []           1.0   \n",
       "\n",
       "                                                 url whitelist_status  wls  \\\n",
       "0  https://www.reddit.com/r/bodyweightfitness/com...          all_ads    6   \n",
       "1  https://www.reddit.com/r/bodyweightfitness/com...          all_ads    6   \n",
       "\n",
       "  post_hint preview author_flair_template_id author_flair_text_color  \\\n",
       "0       NaN     NaN                      NaN                     NaN   \n",
       "1       NaN     NaN                      NaN                     NaN   \n",
       "\n",
       "  author_flair_background_color banned_by link_flair_css_class  \\\n",
       "0                           NaN       NaN                  NaN   \n",
       "1                           NaN       NaN                  NaN   \n",
       "\n",
       "  link_flair_template_id link_flair_text  thumbnail_height  thumbnail_width  \\\n",
       "0                    NaN             NaN               NaN              NaN   \n",
       "1                    NaN             NaN               NaN              NaN   \n",
       "\n",
       "  url_overridden_by_dest crosspost_parent crosspost_parent_list media  \\\n",
       "0                    NaN              NaN                   NaN   NaN   \n",
       "1                    NaN              NaN                   NaN   NaN   \n",
       "\n",
       "  media_embed secure_media secure_media_embed  \n",
       "0         NaN          NaN                NaN  \n",
       "1         NaN          NaN                NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting url to search on\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating weight training dataframe\n",
    "wt_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters to scrape from WeightTraining subreddit\n",
    "params_w = {\n",
    "    'subreddit' : 'WeightTraining',\n",
    "    'size' : 100,\n",
    "    'before' : 1611077619\n",
    "}\n",
    "\n",
    "# Runs and scrapes 100 posts, creates a df, merges w/ prior scrapes\n",
    "res = requests.get(url, params_w)\n",
    "print(f'status code: {res.status_code}')\n",
    "data = res.json()\n",
    "posts = data['data']\n",
    "print(posts[-1]['created_utc'])\n",
    "df = pd.DataFrame(posts)\n",
    "wt_df = pd.concat([wt_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out export so dataframe not saved over\n",
    "# export_df = big_df.to_csv('../data/WT.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating bodyweight dataframe\n",
    "bw_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs and scrapes 100 posts, creates a df, merges w/ prior scrapes\n",
    "# This step is done manually\n",
    "\n",
    "params_b = {\n",
    "    'subreddit' : 'bodyweightfitness',\n",
    "    'size' : 100\n",
    "    ,\n",
    "    'before' : 1611265869\n",
    "}\n",
    "\n",
    "res = requests.get(url, params_b)\n",
    "print(f'status code: {res.status_code}')\n",
    "data = res.json()\n",
    "posts = data['data']\n",
    "print(posts[-1]['created_utc'])\n",
    "df = pd.DataFrame(posts)\n",
    "bw_df = pd.concat([bw_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out export so dataframe not saved over\n",
    "# export_df_bw = bw_df.to_csv('../data/BW.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['subreddit','selftext','title']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-remains",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "valued-mexican",
   "metadata": {},
   "source": [
    "### Gathering data using the PMAW library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-thread",
   "metadata": {},
   "source": [
    "The data collected at this step was not used for analyses. It was, however, compared against the data collected and both methods of scraping were found to be sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mattpodolak/pmaw#parameters\n",
    "# https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "# Instantiating the PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Declaring the subreddit of interest\n",
    "subreddit=\"WeightTraining\"\n",
    "\n",
    "# Setting the maximum number of submissions to be scraped\n",
    "limit=5000\n",
    "\n",
    "# Saving the Submissions\n",
    "submissions = api.search_submissions(subreddit=subreddit, limit=limit, removed_by_category = '')\n",
    "\n",
    "# Turning the WT data into a dataframe\n",
    "submissions_df = pd.DataFrame(submissions)\n",
    "\n",
    "# Exporting the WT dataset to csv\n",
    "# submissions_df.to_csv('../data/WT_pmaw.csv', header=True, index=False, columns=list(submissions_df.axes[1]))\n",
    "\n",
    "submissions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mattpodolak/pmaw#parameters\n",
    "# https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "# Instantiating the PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Declaring the subreddit of interest\n",
    "subreddit=\"bodyweightfitness\"\n",
    "\n",
    "# # Setting the maximum number of submissions to be scraped\n",
    "limit=10000\n",
    "\n",
    "# Saving the Submissions\n",
    "submissions = api.search_submissions(subreddit=subreddit, limit=limit, removed_by_category = '')\n",
    "\n",
    "# Turning the WT data into a dataframe\n",
    "submissions_df = pd.DataFrame(submissions)\n",
    "\n",
    "# Exporting the WT dataset to csv\n",
    "# comments_df.to_csv('../data/BW_pmaw.csv', header=True, index=False, columns=list(comments_df.axes[1]))\n",
    "\n",
    "submissions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-sphere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-syria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
